# -*- coding: utf-8 -*-
"""easyocr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iYGm6BuzSFOAldu-V3wGLNS7enrdpN3I
"""

!conda create -n transformers python=3.9
!pip install transformers[torch]
!pip install tqdm
!pip install Pillow
!pip install opencv-python # NOTE: *not* opencv-contrib-python
!pip install easyocr

pip install seqeval

### Base

import os
import random
from os.path import exists
from os import listdir
from tqdm import tqdm
#from tqdm.notebook import tqdm
import json
import pickle
import pandas as pd
from collections import Counter
import warnings
import numpy as np


### Images
from PIL import Image
from PIL import ImageDraw, ImageFont

### Torch
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
import torchvision
from torchvision.transforms import ToTensor
from torchvision.transforms import ToPILImage
from torchvision.ops import RoIAlign


### Transformers
from transformers import AdamW
from transformers import AutoProcessor

#LayoutLM
from transformers import LayoutLMTokenizer
from transformers import LayoutLMForTokenClassification
from transformers.models.layoutlm import LayoutLMModel, LayoutLMConfig
from transformers.modeling_outputs import TokenClassifierOutput
from transformers import BertTokenizer



#LayoutLMV2
from transformers import LayoutLMv2ForTokenClassification
from transformers import LayoutLMv2Processor

#LayoutLMV3
from transformers import LayoutLMv3ForTokenClassification


### OCR
from easyocr import Reader
import cv2


### For Metrics for model evaluation
from seqeval.metrics import (
    classification_report,
    f1_score,
    precision_score,
    recall_score)


warnings.filterwarnings("ignore")

image = cv2.imread('/content/2.jpg')
# OCR the input image using EasyOCR
reader = Reader(['en'])
results = reader.readtext(image)

# loop over the results
for (bbox, text, prob) in results:
    # display the OCR'd text and associated probability
    print('text : ', text,'\nprobability : ', prob,'\nbounding box : ', bbox , '\n---')
    # unpack the bounding box
    (tl, tr, br, bl) = bbox
    tl = (int(tl[0]), int(tl[1]))
    tr = (int(tr[0]), int(tr[1]))
    br = (int(br[0]), int(br[1]))
    bl = (int(bl[0]), int(bl[1]))
    # cleanup the text and draw the box surrounding the text along
    # with the OCR'd text itself
    #text = cleanup_text(text)
    cv2.rectangle(image, tl, br, (0, 255, 0), 2)
    cv2.putText(image, text, (tl[0], tl[1] - 10),
        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
# show the output image
#cv2.imshow("Image", image)

PIL_image = Image.fromarray(image.astype('uint8'), 'RGB')
PIL_image

### We need to normalise the bounding box co-ordinates for Layout LM
def normalize_bbox(bbox, width, height):
     return [
         int(1000 * (bbox[0] / width)),
         int(1000 * (bbox[1] / height)),
         int(1000 * (bbox[2] / width)),
         int(1000 * (bbox[3] / height)),
     ]



def generate_annotations(path: str, im_path : str):
    image_name = []
    if 'train' in path :

        annotation_files = []
        for js in tqdm(os.listdir(path)[0:300]): ## Used 300 for training
            with open(path + js) as f:
                json_name = path+js
                image_name_ = im_path + json_name.split('/')[-1].split('.')[0] + '.png'
                file_exists = exists(image_name_)
                if file_exists :
                    annotation_files.append(json.load(f))
                    image_name.append(image_name_)
    else:
        annotation_files = []
        for js in tqdm(os.listdir(path)[0:100]): # Used 100 for train and test
            with open(path + js) as f:
                json_name = path+js
                image_name_ = im_path + json_name.split('/')[-1].split('.')[0] + '.png'
                file_exists = exists(image_name_)
                if file_exists :
                    annotation_files.append(json.load(f))
                    image_name.append(image_name_)
    words = []
    boxes = []
    un_boxes = []
    labels = []

    for js in tqdm(annotation_files):
        words_example = []
        boxes_example = []
        un_box_example = []
        labels_example = []

        width, height = js['meta']['image_size']['width'], js['meta']['image_size']['height']
      # loop over OCR annotations
        for elem in js['valid_line']:
            for word in elem['words']:
                # get word
                txt = word['text']
                # get bounding box
                # important: each bounding box should be in (upper left, lower right) format
                # it was confusing to understand the upper left is (x1, y3)
                # and the lower right is (x3, y1)
                x1 = word['quad']['x1']
                y1 = word['quad']['y1']
                x3 = word['quad']['x3']
                y3 = word['quad']['y3']

                box = [x1, y1, x3, y3]
                un_box = box
                box = normalize_bbox(box, width=width, height=height)

                # ADDED
                # skip empty word
                if len(txt) < 1:
                    continue
                if min(box) < 0 or max(box) > 1000: # another bug in which a box had -4
                    continue
                if ((box[3] - box[1]) < 0) or ((box[2] - box[0]) < 0): # another bug in which a box difference was -12
                    continue
                # ADDED

                words_example.append(txt)
                boxes_example.append(box)
                un_box_example.append(un_box)
                labels_example.append(elem['category'])
        words.append(words_example)
        boxes.append(boxes_example)
        un_boxes.append(un_box_example)
        labels.append(labels_example)

    return words, boxes, un_boxes, labels,image_name

words_train, boxes_train,un_boxes_train, labels_train, image_names_train = generate_annotations(train_path_json, train_image_path)
words_val, boxes_val,un_boxes_val, labels_val, image_names_val = generate_annotations(val_path_json, val_image_path)
words_test, boxes_test,un_boxes_test, labels_test,image_names_test = generate_annotations(test_path_json, test_image_path)

